# /my-python-ai-service/main.py
import os
import json
import httpx
import io
import base64
from contextlib import asynccontextmanager
from fastapi import FastAPI, Request
from pydantic import BaseModel
from dotenv import load_dotenv
from google import genai
from google.genai import types

load_dotenv()

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
IMAGEN_ENDPOINT = "https://generativelanguage.googleapis.com/v1beta/models/imagen-4.0-ultra-generate-001:predict"
VEO_BASE_URL = "https://generativelanguage.googleapis.com/v1beta"

AI_PERSONA = os.getenv("AI_PERSONA", """
ë„ˆëŠ” ì‚¬ìš©ìì˜ ì¹œí•œ ì¹œêµ¬ì´ì ìœ ëŠ¥í•œ AI ë¹„ì„œì•¼. 
ì„¤ëª…ì€ ì¹œì ˆí•˜ê³  ê·€ì—½ê²Œ ë°˜ë§(í•´ì²´)ë¡œ í•´ì¤˜. 
ì „ë¬¸ì ì¸ ë‚´ìš©ì´ë¼ë„ ì‰½ê³  ì¬ë¯¸ìˆê²Œ í’€ì–´ì„œ ì„¤ëª…í•´ì¤˜.
""")

client = genai.Client(api_key=GEMINI_API_KEY)

@asynccontextmanager
async def lifespan(app: FastAPI):
    print("INFO:    Shared HTTP Client starting...")
    
    app.state.http_client = httpx.AsyncClient(timeout=120.0)
    yield
    
    await app.state.http_client.aclose()
    print("INFO:    Shared HTTP Client closed.")

app = FastAPI(lifespan=lifespan)

class ImageRequest(BaseModel):
    prompt: str
    aspectRatio: str = "1:1"
    resolution: str = "1K"
    referenceImageUrl: str | None = None
    mimeType: str | None = None

class FilterRequest(BaseModel):
    query: str
    user_id: str
    current_time: str

class DescriptionRequest(BaseModel):
    url: str
    mime_type: str
    file_name: str

class VideoRequest(BaseModel):
    prompt: str

class DeepResearchRequest(BaseModel):
    query: str

class CodeReviewRequest(BaseModel):
    diff: str

async def generate_image_python(request: ImageRequest, http_client: httpx.AsyncClient):
    contents = [request.prompt]
    
    if request.referenceImageUrl:
        try:
            print(f"Downloading reference image: {request.referenceImageUrl}")
            img_resp = await http_client.get(request.referenceImageUrl)
            img_resp.raise_for_status()
            
            image_bytes = io.BytesIO(img_resp.content)
            pil_image = Image.open(image_bytes)
            contents.append(pil_image)
        except Exception as e:
            print(f"Failed to download/process reference image: {e}")

    try:
        response = await client.aio.models.generate_content(
            model="gemini-3-pro-image-preview",
            contents=contents,
            config=types.GenerateContentConfig(
                response_modalities=['IMAGE'],
                image_config=types.ImageConfig(
                    aspect_ratio=request.aspectRatio,
                    image_size=request.resolution
                ),
            )
        )

        base64_images = []
        
        if response.parts:
            for part in response.parts:
                if part.inline_data and part.inline_data.data:
                     raw_bytes = part.inline_data.data
                     b64_str = base64.b64encode(raw_bytes).decode('utf-8')
                     base64_images.append(b64_str)
                elif hasattr(part, 'as_image'):
                     try:
                         img = part.as_image()
                         buffered = io.BytesIO()
                         img.save(buffered, format="PNG")
                         b64_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
                         base64_images.append(b64_str)
                     except Exception as img_err:
                         print(f"Image conversion error: {img_err}")

        if not base64_images:
            raise Exception("AIê°€ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ì§€ ì•Šì•˜ê±°ë‚˜ ì‘ë‹µ í˜•ì‹ì´ ë‹¤ë¦…ë‹ˆë‹¤.")

        return base64_images

    except Exception as e:
        print(f"GenAI Image Generation Error: {e}")
        raise e

@app.post("/generate-image")
async def handle_generate_image(request: ImageRequest, fastapi_req: Request):
    try:
        http_client = fastapi_req.app.state.http_client
        
        base64_images = await generate_image_python(request, http_client)
        return {"status": "success", "images": base64_images}
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/generate-filter")
async def handle_generate_filter(request: FilterRequest):
    prompt = f"""
    You are an expert MongoDB query filter generator.
    (Generate a valid JSON object for MongoDB 'find' operation based on the user query.)
    Respond ONLY with the valid JSON object.
    
    [Task]
    User: "{request.user_id}"
    Query: "{request.query}"
    Current Time: "{request.current_time}"
    """
    try:
        response = client.models.generate_content(
            model='gemini-2.5-pro',
            contents=prompt
        )
        text = response.text
        if "```json" in text: text = text.split("```json")[1].split("```")[0].strip()
        elif "```" in text: text = text.split("```")[1].split("```")[0].strip()
        return json.loads(text)
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/describe-media")
async def handle_describe_media(request: DescriptionRequest, fastapi_req: Request):
    try:
        http_client = fastapi_req.app.state.http_client
        
        file_resp = await http_client.get(request.url)
        file_resp.raise_for_status()
        file_data = file_resp.content

        file_part = types.Part.from_bytes(data=file_data, mime_type=request.mime_type)

        if request.mime_type.startswith('image/'):
            prompt = "ì´ ì´ë¯¸ì§€ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ í•­ëª©ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ê°„ê²°í•˜ê³  ì‚¬ì‹¤ì ìœ¼ë¡œ ë¬˜ì‚¬í•´ ì¤˜. í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì¤˜."
            response = client.models.generate_content(
                model='gemini-2.5-pro',
                contents=[prompt, file_part]
            )
            return {"description": response.text}

        elif request.mime_type.startswith('text/'):
            text_content = file_data.decode('utf-8', errors='ignore')[:4000]
            prompt = f"ì´ í…ìŠ¤íŠ¸ íŒŒì¼({request.file_name}) ë‚´ìš©ì„ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ìš©ìœ¼ë¡œ ìš”ì•½í•´ ì¤˜:\n\n{text_content}"
            response = client.models.generate_content(
                model='gemini-2.5-pro',
                contents=prompt
            )
            return {"description": f"[í…ìŠ¤íŠ¸ íŒŒì¼: {request.file_name}]\n{response.text}"}
        else:
            return {"description": f"(ë¶„ì„ ë¯¸ì§€ì› íŒŒì¼: {request.file_name})"}

    except Exception as e:
        print(f"Description Error: {e}")
        return {"description": f"(AI ë¶„ì„ ì‹¤íŒ¨: {request.file_name})"}

@app.post("/generate-video")
async def handle_generate_video(request: VideoRequest, fastapi_req: Request):
    endpoint = f"{VEO_BASE_URL}/models/veo-3.0-generate-001:predictLongRunning"
    headers = {"Content-Type": "application/json", "x-goog-api-key": GEMINI_API_KEY}
    body = {"instances": [{"prompt": request.prompt}]}
    
    http_client = fastapi_req.app.state.http_client

    try:
        resp = await http_client.post(endpoint, json=body, headers=headers)
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/check-operation/{operation_name:path}")
async def check_operation(operation_name: str, fastapi_req: Request):
    url = f"{VEO_BASE_URL}/{operation_name}"
    headers = {"x-goog-api-key": GEMINI_API_KEY}
    
    http_client = fastapi_req.app.state.http_client

    resp = await http_client.get(url, headers=headers)
    return resp.json()

@app.post("/deep-research")
def handle_deep_research(request: DeepResearchRequest):
    # ê¼¼ê¼¼í•œ ë¶„ì„ê°€ í˜ë¥´ì†Œë‚˜ ì£¼ì… (ê²‰ì€ ê·€ì—½ê²Œ, ì†ì€ ì¹˜ë°€í•˜ê²Œ)
    prompt = f"""
    {AI_PERSONA}
    
    í•˜ì§€ë§Œ ì´ë²ˆ ì‘ì—…ì—ì„œ ë„ˆëŠ” **'ì„¸ê³„ ìµœê³ ì˜ ì‹¬ì¸µ ë¶„ì„ê°€'** ëª¨ë“œë¡œ ì‘ë™í•´ì•¼ í•´.
    ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ëŒ€ì¶© ëŒ€ë‹µí•˜ì§€ ë§ê³ , ì§‘ìš”í•˜ê²Œ íŒŒê³ ë“¤ì–´ì„œ íŒ©íŠ¸ë¥¼ ê²€ì¦í•´ ì¤˜.

    [ì‚¬ìš©ì ìš”ì²­]
    {request.query}

    [ìƒê°ì˜ ì‚¬ìŠ¬ (Chain of Thought) - ì´ ìˆœì„œë¥¼ ë°˜ë“œì‹œ ì§€ì¼œ!]
    1. **Plan**: ë¬´ì—‡ì„ ê²€ìƒ‰í•´ì•¼ ì™„ë²½í•œ ë‹µì„ ì–»ì„ ìˆ˜ ìˆì„ì§€ ì „ëµì„ ì„¸ìš´ë‹¤.
    2. **Search & Analyze**: Google Searchë¥¼ í†µí•´ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³  ë¶„ì„í•œë‹¤.
    3. **Critique (ë¹„íŒì  ì¬ê²€í† )**: ìˆ˜ì§‘í•œ ì •ë³´ì— ë¶€ì¡±í•œ ì ì€ ì—†ëŠ”ì§€, í¸í–¥ë˜ì§€ëŠ” ì•Šì•˜ëŠ”ì§€ ìŠ¤ìŠ¤ë¡œ ë°˜ë¬¸í•˜ê³  ë³´ì™„í•œë‹¤.
    4. **Drafting**: ìˆ˜ì§‘ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°©ëŒ€í•œ ì–‘ì˜ 'ìƒì„¸ ë³´ê³ ì„œ'ì™€ í•µì‹¬ë§Œ ìš”ì•½í•œ 'ë¸Œë¦¬í•‘'ì„ ì‘ì„±í•œë‹¤.

    [ìµœì¢… ì¶œë ¥ í˜•ì‹ (Strict Output Format)]
    ë¶„ì„ì´ ëë‚˜ë©´ ë°˜ë“œì‹œ ì•„ë˜ XML íƒœê·¸ í˜•ì‹ì„ ì—„ê²©í•˜ê²Œ ì§€ì¼œì„œ ë‹µë³€í•´. 
    ë‹¤ë¥¸ ì¡ë‹´ì€ íƒœê·¸ ë°–ì— ì“°ì§€ ë§ˆ.

    <REPORT_FILE>
    # ğŸ“‘ ì‹¬ì¸µ ë¦¬ì„œì¹˜ ë³´ê³ ì„œ: [ì£¼ì œ]
    (ì—¬ê¸°ì— ë§ˆí¬ë‹¤ìš´(Markdown) í˜•ì‹ìœ¼ë¡œ ì•„ì£¼ ìƒì„¸í•˜ê²Œ ì‘ì„±í•´. ë…¼ë¬¸ ìˆ˜ì¤€ìœ¼ë¡œ ê¹Šì´ ìˆê²Œ. 
    ì¶œì²˜(Source) ë§í¬ë„ ê¼¼ê¼¼í•˜ê²Œ ë‹¬ì•„ì¤˜. ê¸¸ì´ ì œí•œ ì—†ì´ ë§ˆìŒê» ì¨ë„ ë¼.)
    </REPORT_FILE>

    <DISCORD_EMBED>
    (ì—¬ê¸°ì— ë””ìŠ¤ì½”ë“œ ì±„íŒ…ì°½ì— ë³´ì—¬ì¤„ ë‚´ìš©ì„ ì‘ì„±í•´.)
    - **ë¶„ëŸ‰**: 300ì~500ì ì´ë‚´.
    - **ë§íˆ¬**: ë„ˆì˜ ì›ë˜ í˜ë¥´ì†Œë‚˜(ê·€ì—¬ìš´ ë°˜ë§)ë¥¼ ìœ ì§€í•´. ì´ëª¨ì§€ëŠ” ë°©í•´ë˜ì§€ ì•Šì„ë§Œí¼ ì ë‹¹íˆ í™œìš©!
    - **ë‚´ìš©**: 
      1. ì¡°ì‚¬ë¥¼ í†µí•´ ì•Œì•„ë‚¸ ê°€ì¥ ì¶©ê²©ì ì´ê±°ë‚˜ ì¤‘ìš”í•œ 3ê°€ì§€ í¬ì¸íŠ¸ (ê¸€ë¨¸ë¦¬ ê¸°í˜¸)
      2. ë„ˆì˜ í•œ ì¤„ ì´í‰
      3. "ìì„¸í•œ ë‚´ìš©ì€ ìœ„ì— ì²¨ë¶€í•œ íŒŒì¼ ì½ì–´ë´! ğŸ“„" ë¼ëŠ” ë©˜íŠ¸ë¡œ ë§ˆë¬´ë¦¬.
    </DISCORD_EMBED>
    """

    try:
        grounding_tool = types.Tool(google_search=types.GoogleSearch())
        
        response = client.models.generate_content(
            model='gemini-2.5-pro', 
            contents=prompt,
            config=types.GenerateContentConfig(
                tools=[grounding_tool],
                response_modalities=["TEXT"],
                temperature=0.4
            )
        )
        
        return {"status": "success", "report": response.text}

    except Exception as e:
        print(f"Deep Research Error: {e}")
        return {"status": "error", "message": str(e)}

@app.post("/code-review")
def handle_code_review(request: CodeReviewRequest):
    prompt = f"""
    {AI_PERSONA}
    
    ì´ë²ˆì—” **'Google ìˆ˜ì„ ì—”ì§€ë‹ˆì–´ ê²¸ ë³´ì•ˆ ì „ë¬¸ê°€'** ëª¨ë“œì•¼.
    ì•„ë˜ ì œê³µëœ [Git Diff]ëŠ” ì§€ë‚œ ì¼ì£¼ì¼ ë™ì•ˆ ë³€ê²½ëœ ì„œë²„ ì½”ë“œì•¼. 
    ì´ ë³€ê²½ ì‚¬í•­ë“¤ì„ ì•„ì£¼ ê¼¼ê¼¼í•˜ê²Œ ì ê²€í•´ì¤˜.

    [Git Diff (ì´ë²ˆ ì£¼ ë³€ê²½ ì‚¬í•­)]
    {request.diff}

    [ìƒê°ì˜ ì‚¬ìŠ¬ (Chain of Thought)]
    1. **Scan**: ë³€ê²½ëœ íŒŒì¼ê³¼ ë¡œì§ì˜ ì˜ë„ë¥¼ ë¨¼ì € íŒŒì•…í•œë‹¤.
    2. **Deep Dive**: 
       - ğŸ› ë²„ê·¸ ê°€ëŠ¥ì„±: ì—£ì§€ ì¼€ì´ìŠ¤(Edge case) ì²˜ë¦¬ ë¯¸í¡, íƒ€ì… ì—ëŸ¬ ë“±.
       - ğŸ›¡ï¸ ë³´ì•ˆ ì·¨ì•½ì : SQL Injection, XSS, ë¯¼ê° ì •ë³´ ë…¸ì¶œ ë“±.
       - âš¡ ì„±ëŠ¥ ì´ìŠˆ: ë¶ˆí•„ìš”í•œ ë£¨í”„, ë©”ëª¨ë¦¬ ëˆ„ìˆ˜, ë¹„íš¨ìœ¨ì ì¸ DB ì¿¼ë¦¬.
       - ğŸ§¹ ê°€ë…ì„±: ë³€ìˆ˜ëª…, í•¨ìˆ˜ êµ¬ì¡°, ì¤‘ë³µ ì½”ë“œ.
    3. **Critique**: "ì´ê²Œ ìµœì„ ì¸ê°€?" ìŠ¤ìŠ¤ë¡œ ë°˜ë¬¸í•˜ë©° ë” ë‚˜ì€ ëŒ€ì•ˆ(Best Practice)ì„ ìƒê°í•œë‹¤.
    4. **Drafting**: íŒŒì¼ìš© 'ìƒì„¸ ë¦¬í¬íŠ¸'ì™€ ë””ìŠ¤ì½”ë“œìš© 'ìš”ì•½ë³¸'ì„ ì‘ì„±í•œë‹¤.

    [ìµœì¢… ì¶œë ¥ í˜•ì‹ (Strict Output Format)]
    ë°˜ë“œì‹œ ì•„ë˜ íƒœê·¸ í˜•ì‹ì„ ì§€ì¼œì„œ ì¶œë ¥í•´.

    <REPORT_FILE>
    # ğŸ“… ì£¼ê°„ ì½”ë“œ ë¦¬ë·° ë¦¬í¬íŠ¸
    ## 1. ì´í‰
    ## 2. ì£¼ìš” ë³€ê²½ ì‚¬í•­ ë¶„ì„
    ## 3. ğŸš¨ ë°œê²¬ëœ ë¬¸ì œì  ë° ê°œì„  ì œì•ˆ
    (ì—¬ê¸°ì— ì½”ë“œ ë¸”ë¡ê³¼ í•¨ê»˜ ì•„ì£¼ ìƒì„¸í•˜ê²Œ ì‘ì„±í•´. ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• í™œìš©.)
    </REPORT_FILE>

    <DISCORD_EMBED>
    (ë””ìŠ¤ì½”ë“œ ì„ë² ë“œìš© ìš”ì•½. 500ì ì´ë‚´.)
    - **ë§íˆ¬**: í‰ì†Œì˜ ì¹œê·¼í•œ ë§íˆ¬ ìœ ì§€.
    - **ë‚´ìš©**:
      1. ì´ë²ˆ ì£¼ ë³€ê²½ëœ íŒŒì¼ ê°œìˆ˜ ë° ì£¼ìš” ì‘ì—… ìš”ì•½ (í•œ ì¤„)
      2. ì¹­ì°¬í•  ì  ğŸ‘ (ì—†ìœ¼ë©´ ìƒëµ)
      3. ê³ ì³ì•¼ í•  ì  ğŸ› ï¸ (ê°€ì¥ ì¹˜ëª…ì ì¸ ê²ƒ 1~2ê°œë§Œ)
      4. "ìƒì„¸í•œ ê±´ íŒŒì¼ ì—´ì–´ì„œ í™•ì¸í•´! í”¼ë“œë°± ë°˜ì˜ ë¶€íƒí•´~ ğŸ˜‰"
    </DISCORD_EMBED>
    """

    try:
        # ì½”ë“œ ë¦¬ë·°ëŠ” ê¸´ ë¬¸ë§¥ ì²˜ë¦¬ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ Pro ëª¨ë¸ ì‚¬ìš© ê¶Œì¥
        response = client.models.generate_content(
            model='gemini-2.5-pro',
            contents=prompt,
            config=types.GenerateContentConfig(
                temperature=0.2 # ì½”ë“œëŠ” ì°½ì˜ì„±ë³´ë‹¤ ì •í™•ì„±ì´ ìƒëª…! ì˜¨ë„ë¥¼ ë‚®ì¶¤.
            )
        )
        return {"status": "success", "report": response.text}

    except Exception as e:
        print(f"Code Review Error: {e}")
        return {"status": "error", "message": str(e)}

@app.get("/")
def read_root():
    return {"status": "Python AI Service is running!"}